{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIx2P25UxP7h"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the directory containing your images in Google Drive\n",
        "drive_directory_path = '/content/drive/My Drive/JPG'\n",
        "\n",
        "# Data Preparation for Images\n",
        "data_dir = '/content/drive/My Drive/JPG'  # Path to images folder in Drive\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "##############\n",
        "############## THIS SECTION IS OPTIONAL IF YOU HAVENT PREPROCESSED THE DATASET YET\n",
        "for filename in os.listdir(data_dir):\n",
        "    if filename.endswith('.png'):\n",
        "        img = cv2.imread(os.path.join(data_dir, filename), cv2.IMREAD_GRAYSCALE)\n",
        "        img = cv2.resize(img, (100, 100))  # Resize image to desired dimensions\n",
        "        img = img / 255.0  # Normalize pixel values\n",
        "        images.append(img)\n",
        "\n",
        "\n",
        "###############\n",
        "\n",
        "        # Label processing - 1 for presence of human head, 0 for absence\n",
        "        label = 1 if 'human_head' in filename else 0\n",
        "        labels.append(label)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split data into train, validation, test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
        "\n",
        "# Build a more complex CNN Model with modified dropout rates\n",
        "output_classes = 2  # Number of output classes (binary in this case)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(100, 100, 1), kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.9),  # Higher dropout rate\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.6),  # Moderate dropout rate\n",
        "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.8),  # Higher dropout rate\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.5),  # Standard dropout rate\n",
        "    tf.keras.layers.Dense(output_classes, activation='softmax')  # Softmax activation for multi-class\n",
        "])\n",
        "\n",
        "# Compile the model for multi-class classification\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "model.fit(X_train[..., np.newaxis], y_train, epochs=15, batch_size=64, validation_data=(X_val[..., np.newaxis], y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test[..., np.newaxis], y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Quantize the model\n",
        "quantized_model = tf.keras.models.clone_model(model)\n",
        "quantized_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "def representative_data_gen():\n",
        "    for input_value in tf.data.Dataset.from_tensor_slices(X_train[..., np.newaxis]).batch(1).take(100):\n",
        "        yield [input_value]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(quantized_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "quantized_tflite_model = converter.convert()\n",
        "\n",
        "# Save the quantized model to a file\n",
        "with open('/content/drive/My Drive/quantized_model.tflite', 'wb') as f:\n",
        "    f.write(quantized_tflite_model)\n"
      ]
    }
  ]
}