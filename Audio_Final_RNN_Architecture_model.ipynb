{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRhnluog1NJu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from scipy import signal\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Path to the directory containing the audio files in Google Drive\n",
        "drive_directory_path = '/content/drive/My Drive/WAV'\n",
        "\n",
        "# Filter for WAV files in the directory\n",
        "audio_files = [os.path.join(drive_directory_path, file) for file in os.listdir(drive_directory_path) if file.endswith('.wav')]\n",
        "\n",
        "# Count the number of WAV files found\n",
        "num_files = len(audio_files)\n",
        "print(f\"Number of WAV files found: {num_files}\")\n",
        "\n",
        "# Constants TAKEN FROM EXAMPLE IN BANDPASS FILTERING AND PREPROCESSING BY KAGGLE PROJECT\n",
        "gSampleRate = 7000\n",
        "upperCutoffFreq = 3000\n",
        "cutoffFrequencies = [80, upperCutoffFreq]\n",
        "\n",
        "## PREPROCESSING FEATURE EXTRACTION IN DATA DEVELOPMENT\n",
        "\n",
        "# Load and preprocess audio files\n",
        "def load_and_preprocess_files(audio_file_paths):\n",
        "\n",
        "\n",
        "# Extracting features\n",
        "spectrograms = [calculate_spectrogram(audio, gSampleRate)[0] for audio in audioBuffers]\n",
        "mfccs = [calculate_mfcc(audio, gSampleRate) for audio in audioBuffers]\n",
        "chromagrams = [calculate_chromagram(audio, gSampleRate) for audio in audioBuffers]\n",
        "\n",
        "# Reshape the features if  to  the same number of frames (time )\n",
        "min_frames = min(spec.shape[1] for spec in spectrograms)\n",
        "spectrograms = [spec[:, :min_frames] for spec in spectrograms]\n",
        "mfccs = [mfcc[:, :min_frames] for mfcc in mfccs]\n",
        "chromagrams = [chroma[:, :min_frames] for chroma in chromagrams]\n",
        "\n",
        "# Convert features to numpy arrays and transpose them to have frames as rows and features as columns\n",
        "spectrograms_np = np.array([spec.T for spec in spectrograms])\n",
        "mfccs_np = np.array([mfcc.T for mfcc in mfccs])\n",
        "chromagrams_np = np.array([chroma.T for chroma in chromagrams])\n",
        "\n",
        "#  features as one input array\n",
        "combined_features = np.concatenate((spectrograms_np, mfccs_np, chromagrams_np), axis=2)\n",
        "print(combined_features.shape)\n",
        "\n",
        "\n",
        "# Build an RNN model NOT ROBUST ENOUGH (THIS IS INITIAL ITERATION)\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(combined_features.shape[1], combined_features.shape[2])))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Reshape features for RNN input (adding the time dimension)\n",
        "combined_features = np.reshape(combined_features, (combined_features.shape[0], combined_features.shape[1], combined_features.shape[2]))\n",
        "\n",
        "# Train the model\n",
        "model.fit(combined_features, labels, epochs=10, batch_size=32, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuBZAAj21Qy2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, LeakyReLU, BatchNormalization\n",
        "\n",
        "# Features should already be prepared\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# LSTM layers with increased units and stacking for deeper learning & complexity in pattern recognition\n",
        "model.add(LSTM(256, input_shape=(combined_features.shape[1], combined_features.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.8))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Additional dense layers for better pass through learning\n",
        "model.add(Dense(128))\n",
        "model.add(ReLU(alpha=0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Dense(64))\n",
        "model.add(LeakyReLU(alpha=0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "# Output layer with sigmoid activation for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Reshape features for RNN input (adding the time dimension)\n",
        "combined_features = np.reshape(combined_features, (combined_features.shape[0], combined_features.shape[1], 1))\n",
        "\n",
        "# Train the model\n",
        "model.fit(combined_features, labels, epochs=350, batch_size=64, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pruning  the model\n",
        "def apply_pruning_to_model(model):\n",
        "    pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(\n",
        "        initial_sparsity=0.00, final_sparsity=0.50, begin_step=0, end_step=1000\n",
        "    )\n",
        "    pruning_callbacks = [\n",
        "        tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "        tfmot.sparsity.keras.PruningSummaries(log_dir='/content/drive/My Drive/pruning_logs'),\n",
        "    ]\n",
        "    model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, pruning_schedule=pruning_schedule)\n",
        "    model_for_pruning.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model_for_pruning\n",
        "\n",
        "## NEW MODEL WILL BE QUANTIZED\n",
        "pruned_model = apply_pruning_to_model(model)\n",
        "\n",
        "# BALANCE THE WEIGHTS (KAGGLE SOURCE FOR AUDIO)\n",
        "pruned_model.fit(combined_features, labels, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Convert 2 a quantized version\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "quantized_tflite_model = converter.convert()\n",
        "\n",
        "# Save  quantized model to a file\n",
        "with open('/content/drive/My Drive/quantized_audio_model_optimized.tflite', 'wb') as f:\n",
        "    f.write(audio_model)\n",
        "## SAVE AS A .H FILE\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
